{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e02de6",
   "metadata": {},
   "source": [
    "# Web App Data Prep\n",
    "\n",
    "This notebook prepares the cleaned flight dataset for the web app database. It builds dimension tables (airlines, airports, aircraft types) and fact tables (routes and route metrics), then writes Postgres DDL and seed INSERT statements into `./sql_statements`.\n",
    "\n",
    "Data source: `./clean_data/final_flight_data.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be6982",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "Install required Python packages for data wrangling and file generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c8d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac87d28",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Read the cleaned flight dataset once so all export steps share the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c029016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import uuid\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "df = pd.read_csv('./clean_data/final_flight_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74870b7",
   "metadata": {},
   "source": [
    "## Airlines table and seed data\n",
    "Filter and normalize airline fields, then emit Postgres DDL and INSERT statements for the `airlines` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cceec8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote SQL files to: /Users/darius/Documents/projects/portfolio/flighter/data-mining/sql_statements\n",
      "Airlines rows exported: 174\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — filter/normalize airlines from df + generate Postgres SQL files into ./sql_statements\n",
    "\n",
    "\n",
    "# df = ...  # your raw dataframe\n",
    "\n",
    "SQL_DIR = \"./sql_statements\"\n",
    "os.makedirs(SQL_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Extract & normalize airline dimension\n",
    "airlines_df = (\n",
    "    df[[\n",
    "        \"airline_id\",\n",
    "        \"airline_name\",\n",
    "        \"airline_iata\",\n",
    "        \"airline_icao\",\n",
    "        \"airline_call_sign\",\n",
    "        \"airline_country\",\n",
    "        \"airline_active\",\n",
    "    ]]\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# cleanup\n",
    "for c in [\"airline_name\", \"airline_iata\", \"airline_icao\", \"airline_call_sign\", \"airline_country\"]:\n",
    "    airlines_df[c] = airlines_df[c].astype(\"string\").str.strip().replace({\"\": pd.NA})\n",
    "\n",
    "airlines_df[\"airline_active\"] = airlines_df[\"airline_active\"].fillna(False).astype(bool)\n",
    "\n",
    "# recommended filters: active only + require at least one code\n",
    "airlines_df = airlines_df[\n",
    "    (airlines_df[\"airline_active\"])\n",
    "    & (airlines_df[\"airline_iata\"].notna() | airlines_df[\"airline_icao\"].notna())\n",
    "].copy()\n",
    "\n",
    "# uppercase codes\n",
    "for c in [\"airline_iata\", \"airline_icao\"]:\n",
    "    airlines_df[c] = airlines_df[c].str.upper()\n",
    "\n",
    "# 2) Generate SQL files (DDL + inserts)\n",
    "ddl_sql = \"\"\"\\\n",
    "-- airlines table (Postgres)\n",
    "CREATE TABLE IF NOT EXISTS airlines (\n",
    "  airline_id   TEXT PRIMARY KEY,\n",
    "  name         TEXT,\n",
    "  iata_code    TEXT,\n",
    "  icao_code    TEXT,\n",
    "  call_sign    TEXT,\n",
    "  country      TEXT,\n",
    "  is_active    BOOLEAN NOT NULL DEFAULT TRUE,\n",
    "  logo_path    TEXT\n",
    ");\n",
    "\n",
    "CREATE UNIQUE INDEX IF NOT EXISTS airlines_iata_code_uq\n",
    "  ON airlines (iata_code) WHERE iata_code IS NOT NULL;\n",
    "\n",
    "CREATE UNIQUE INDEX IF NOT EXISTS airlines_icao_code_uq\n",
    "  ON airlines (icao_code) WHERE icao_code IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "def sql_quote(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (isinstance(v, pd._libs.missing.NAType)):\n",
    "        return \"NULL\"\n",
    "    s = str(v).replace(\"'\", \"''\")\n",
    "    return f\"'{s}'\"\n",
    "\n",
    "insert_rows = []\n",
    "for r in airlines_df.itertuples(index=False):\n",
    "    insert_rows.append(\n",
    "        \"INSERT INTO airlines (airline_id, name, iata_code, icao_code, call_sign, country, is_active)\\n\"\n",
    "        f\"VALUES ({sql_quote(r.airline_id)}, {sql_quote(r.airline_name)}, {sql_quote(r.airline_iata)}, \"\n",
    "        f\"{sql_quote(r.airline_icao)}, {sql_quote(r.airline_call_sign)}, {sql_quote(r.airline_country)}, \"\n",
    "        f\"{'TRUE' if bool(r.airline_active) else 'FALSE'});\\n\"\n",
    "    )\n",
    "\n",
    "inserts_sql = \"-- airlines seed data\\n\" + \"\".join(insert_rows)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"001_create_airlines.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(ddl_sql)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"002_insert_airlines.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(inserts_sql)\n",
    "\n",
    "print(f\"Wrote SQL files to: {os.path.abspath(SQL_DIR)}\")\n",
    "print(f\"Airlines rows exported: {len(airlines_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef6bad",
   "metadata": {},
   "source": [
    "## Optional: download airline logos\n",
    "Fetch airline logos into `./airline_images` for UI use. This cell is intentionally skipped by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bdabce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping airline logo downloads, run manually as needed\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping airline logo downloads, run manually as needed\n",
    "# Cell 2 — download airline logos to ./airline_images (no DB writes), only run manually as needed\n",
    "\n",
    "\n",
    "\n",
    "IMG_DIR = \"./airline_images\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# Use the filtered airlines_df from Cell 1\n",
    "# We'll primarily use IATA; fall back to ICAO only for filename (many logo sources are IATA-based)\n",
    "def safe_code(x: str) -> str | None:\n",
    "    if x is None or pd.isna(x):\n",
    "        return None\n",
    "    x = str(x).strip().upper()\n",
    "    return x if re.fullmatch(r\"[A-Z0-9]{2,4}\", x) else None\n",
    "\n",
    "# Aviasales/Travelpayouts logo URL pattern (IATA-based, PNG)\n",
    "def aviasales_logo_url(iata: str, size: int = 200) -> str:\n",
    "    return f\"http://img.wway.io/pics/root/{iata}@png?exar=1&rs=fit:{size}:{size}\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"flight-booking-app/0.1 (logo fetcher)\"})\n",
    "\n",
    "downloaded = 0\n",
    "skipped = 0\n",
    "failed = 0\n",
    "\n",
    "for r in airlines_df.itertuples(index=False):\n",
    "    iata = safe_code(r.airline_iata)\n",
    "    icao = safe_code(r.airline_icao)\n",
    "\n",
    "    # this source is IATA-based; skip if no IATA\n",
    "    if not iata:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    out_path = os.path.join(IMG_DIR, f\"{iata}.png\")\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        url = aviasales_logo_url(iata, size=200)\n",
    "        resp = session.get(url, timeout=20)\n",
    "\n",
    "        if resp.status_code != 200 or not resp.content:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"image\" not in ctype:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "\n",
    "        downloaded += 1\n",
    "        time.sleep(0.05)  # be polite\n",
    "\n",
    "    except Exception:\n",
    "        failed += 1\n",
    "\n",
    "print(f\"Saved logos to: {os.path.abspath(IMG_DIR)}\")\n",
    "print(f\"Downloaded: {downloaded:,} | Skipped: {skipped:,} | Failed: {failed:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ea28e",
   "metadata": {},
   "source": [
    "## Airports table and seed data\n",
    "Create a unified airports dimension from both source and destination columns, then write DDL and INSERTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c640c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote airports SQL to: /Users/darius/Documents/projects/portfolio/flighter/data-mining/sql_statements | rows: 1,372\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — generate Postgres SQL for airports into ./sql_statements\n",
    "\n",
    "SQL_DIR = \"./sql_statements\"\n",
    "os.makedirs(SQL_DIR, exist_ok=True)\n",
    "\n",
    "def sql_quote(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    s = str(v).replace(\"'\", \"''\")\n",
    "    return f\"'{s}'\"\n",
    "\n",
    "def to_bool(v):\n",
    "    if pd.isna(v):\n",
    "        return \"NULL\"\n",
    "    return \"TRUE\" if bool(v) else \"FALSE\"\n",
    "\n",
    "def to_num(v):\n",
    "    if pd.isna(v) or v is None:\n",
    "        return \"NULL\"\n",
    "    return str(v)\n",
    "\n",
    "# ---- Build airports dimension from BOTH source + destination columns\n",
    "src_airports = pd.DataFrame({\n",
    "    \"airport_id\": df[\"route_source_airport_id\"],\n",
    "    \"name\": df[\"source_port_name\"],\n",
    "    \"city\": df[\"source_port_city\"],\n",
    "    \"country\": df[\"source_port_country\"],\n",
    "    \"iata_code\": df[\"source_port_iata\"],\n",
    "    \"icao_code\": df[\"source_port_icao\"],\n",
    "    \"latitude\": df[\"source_port_latitude\"],\n",
    "    \"longitude\": df[\"source_port_longitude\"],\n",
    "    \"timezone\": df[\"source_port_timezone\"],\n",
    "    \"database_timezone\": df[\"source_port_database_timezone\"],\n",
    "    \"type\": df[\"source_Type\"],\n",
    "})\n",
    "\n",
    "dst_airports = pd.DataFrame({\n",
    "    \"airport_id\": df[\"route_destination_airport_id\"],\n",
    "    \"name\": df[\"destination_port_name\"],\n",
    "    \"city\": df[\"destination_port_city\"],\n",
    "    \"country\": df[\"destination_port_country\"],\n",
    "    \"iata_code\": df[\"destination_port_iata\"],\n",
    "    \"icao_code\": df[\"destination_port_icao\"],\n",
    "    \"latitude\": df[\"destination_port_latitude\"],\n",
    "    \"longitude\": df[\"destination_port_longitude\"],\n",
    "    \"timezone\": df[\"destination_port_timezone\"],\n",
    "    \"database_timezone\": df[\"destination_port_database_timezone\"],\n",
    "    \"type\": df[\"destination_Type\"],\n",
    "})\n",
    "\n",
    "airports_df = (\n",
    "    pd.concat([src_airports, dst_airports], ignore_index=True)\n",
    "    .drop_duplicates(subset=[\"airport_id\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# cleanup\n",
    "for c in [\"name\",\"city\",\"country\",\"iata_code\",\"icao_code\",\"timezone\",\"database_timezone\",\"type\"]:\n",
    "    airports_df[c] = airports_df[c].astype(\"string\").str.strip().replace({\"\": pd.NA})\n",
    "for c in [\"iata_code\",\"icao_code\"]:\n",
    "    airports_df[c] = airports_df[c].str.upper()\n",
    "\n",
    "# ---- DDL\n",
    "airports_ddl = \"\"\"\\\n",
    "-- airports table (Postgres)\n",
    "CREATE TABLE IF NOT EXISTS airports (\n",
    "  airport_id        BIGINT PRIMARY KEY,\n",
    "  name              TEXT,\n",
    "  city              TEXT,\n",
    "  country           TEXT,\n",
    "  iata_code          TEXT,\n",
    "  icao_code          TEXT,\n",
    "  latitude          DOUBLE PRECISION,\n",
    "  longitude         DOUBLE PRECISION,\n",
    "  timezone          TEXT,\n",
    "  database_timezone TEXT,\n",
    "  type              TEXT\n",
    ");\n",
    "\n",
    "CREATE UNIQUE INDEX IF NOT EXISTS airports_iata_code_uq\n",
    "  ON airports (iata_code) WHERE iata_code IS NOT NULL;\n",
    "\n",
    "CREATE UNIQUE INDEX IF NOT EXISTS airports_icao_code_uq\n",
    "  ON airports (icao_code) WHERE icao_code IS NOT NULL;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS airports_country_city_idx\n",
    "  ON airports (country, city);\n",
    "\"\"\"\n",
    "\n",
    "# ---- INSERTs\n",
    "rows = []\n",
    "for r in airports_df.itertuples(index=False):\n",
    "    rows.append(\n",
    "        \"INSERT INTO airports (airport_id, name, city, country, iata_code, icao_code, latitude, longitude, timezone, database_timezone, type)\\n\"\n",
    "        f\"VALUES ({to_num(r.airport_id)}, {sql_quote(r.name)}, {sql_quote(r.city)}, {sql_quote(r.country)}, \"\n",
    "        f\"{sql_quote(r.iata_code)}, {sql_quote(r.icao_code)}, {to_num(r.latitude)}, {to_num(r.longitude)}, \"\n",
    "        f\"{sql_quote(r.timezone)}, {sql_quote(r.database_timezone)}, {sql_quote(r.type)});\\n\"\n",
    "    )\n",
    "\n",
    "airports_inserts = \"-- airports seed data\\n\" + \"\".join(rows)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"003_create_airports.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(airports_ddl)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"004_insert_airports.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(airports_inserts)\n",
    "\n",
    "print(f\"Wrote airports SQL to: {os.path.abspath(SQL_DIR)} | rows: {len(airports_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ef502",
   "metadata": {},
   "source": [
    "## Aircraft types, routes, and route metrics\n",
    "Generate aircraft type rows, deterministic route IDs, and route metrics, then write all related SQL files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb186629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote aircraft_types SQL | rows: 29\n",
      "Wrote routes + route_metrics SQL to: /Users/darius/Documents/projects/portfolio/flighter/data-mining/sql_statements\n",
      "Routes rows: 20,471 | Route metrics rows: 20,471\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — generate Postgres SQL for aircraft_types (planes), routes, and route_metrics into ./sql_statements\n",
    "\n",
    "SQL_DIR = \"./sql_statements\"\n",
    "os.makedirs(SQL_DIR, exist_ok=True)\n",
    "\n",
    "def sql_quote(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    s = str(v).replace(\"'\", \"''\")\n",
    "    return f\"'{s}'\"\n",
    "\n",
    "def to_num(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    return str(v)\n",
    "\n",
    "def to_bool(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    return \"TRUE\" if bool(v) else \"FALSE\"\n",
    "\n",
    "def stable_uuid_from_key(key: str) -> str:\n",
    "    \"\"\"Deterministic UUID from a string key.\"\"\"\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_URL, key))\n",
    "\n",
    "def stable_route_id(airline_id, src_id, dst_id, stops, plane_iso):\n",
    "    # deterministic id (UUID) from a hash of natural keys\n",
    "    key = f\"ROUTE|{airline_id}|{src_id}|{dst_id}|{stops}|{plane_iso}\"\n",
    "    return stable_uuid_from_key(key)\n",
    "\n",
    "# -------------------------\n",
    "# aircraft_types (planes)\n",
    "# -------------------------\n",
    "planes_df = (\n",
    "    df[[\n",
    "        \"plane_iso\",\n",
    "        \"plane_name\",\n",
    "        \"aircraft_name\",\n",
    "        \"manufacturer\",\n",
    "        \"category\",\n",
    "        \"fuel_litre_per_100km_per_passenger\",\n",
    "        \"capacity_min\",\n",
    "        \"capacity_max\",\n",
    "        \"range_nm\",\n",
    "        \"co2_g_per_pax_mile\",\n",
    "    ]]\n",
    "    .drop_duplicates(subset=[\"plane_iso\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "for c in [\"plane_iso\",\"plane_name\",\"aircraft_name\",\"manufacturer\",\"category\"]:\n",
    "    planes_df[c] = planes_df[c].astype(\"string\").str.strip().replace({\"\": pd.NA})\n",
    "planes_df[\"plane_iso\"] = planes_df[\"plane_iso\"].str.upper()\n",
    "\n",
    "planes_ddl = \"\"\"\\\n",
    "-- aircraft_types table (Postgres)\n",
    "CREATE TABLE IF NOT EXISTS aircraft_types (\n",
    "  plane_iso                         TEXT PRIMARY KEY,\n",
    "  plane_name                        TEXT,\n",
    "  aircraft_name                     TEXT,\n",
    "  manufacturer                      TEXT,\n",
    "  category                          TEXT,\n",
    "  fuel_litre_per_100km_per_passenger DOUBLE PRECISION,\n",
    "  capacity_min                      INTEGER,\n",
    "  capacity_max                      INTEGER,\n",
    "  range_nm                          INTEGER,\n",
    "  co2_g_per_pax_mile                DOUBLE PRECISION\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS aircraft_types_mfr_cat_idx\n",
    "  ON aircraft_types (manufacturer, category);\n",
    "\"\"\"\n",
    "\n",
    "planes_rows = []\n",
    "for r in planes_df.itertuples(index=False):\n",
    "    planes_rows.append(\n",
    "        \"INSERT INTO aircraft_types (plane_iso, plane_name, aircraft_name, manufacturer, category, \"\n",
    "        \"fuel_litre_per_100km_per_passenger, capacity_min, capacity_max, range_nm, co2_g_per_pax_mile)\\n\"\n",
    "        f\"VALUES ({sql_quote(r.plane_iso)}, {sql_quote(r.plane_name)}, {sql_quote(r.aircraft_name)}, \"\n",
    "        f\"{sql_quote(r.manufacturer)}, {sql_quote(r.category)}, {to_num(r.fuel_litre_per_100km_per_passenger)}, \"\n",
    "        f\"{to_num(r.capacity_min)}, {to_num(r.capacity_max)}, {to_num(r.range_nm)}, {to_num(r.co2_g_per_pax_mile)});\\n\"\n",
    "    )\n",
    "\n",
    "planes_inserts = \"-- aircraft_types seed data\\n\" + \"\".join(planes_rows)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"005_create_aircraft_types.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(planes_ddl)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"006_insert_aircraft_types.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(planes_inserts)\n",
    "\n",
    "print(f\"Wrote aircraft_types SQL | rows: {len(planes_df):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# routes + route_metrics\n",
    "# -------------------------\n",
    "routes_cols = [\n",
    "    \"airline_id\",\n",
    "    \"route_source_airport_id\",\n",
    "    \"route_destination_airport_id\",\n",
    "    \"route_stops\",\n",
    "    \"route_plane_iso\",\n",
    "    \"distance_km\",\n",
    "    \"distance_miles\",\n",
    "    \"is_international\",\n",
    "    \"co2_total_kg\",\n",
    "]\n",
    "\n",
    "routes_raw = df[routes_cols].drop_duplicates().copy()\n",
    "\n",
    "# normalize codes\n",
    "routes_raw[\"route_plane_iso\"] = routes_raw[\"route_plane_iso\"].astype(\"string\").str.strip().str.upper()\n",
    "routes_raw[\"airline_id\"] = routes_raw[\"airline_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "# deterministic route_id\n",
    "routes_raw[\"route_id\"] = routes_raw.apply(\n",
    "    lambda x: stable_route_id(\n",
    "        x[\"airline_id\"],\n",
    "        x[\"route_source_airport_id\"],\n",
    "        x[\"route_destination_airport_id\"],\n",
    "        x[\"route_stops\"],\n",
    "        x[\"route_plane_iso\"],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "routes_df = routes_raw[[\n",
    "    \"route_id\",\n",
    "    \"airline_id\",\n",
    "    \"route_source_airport_id\",\n",
    "    \"route_destination_airport_id\",\n",
    "    \"route_stops\",\n",
    "    \"route_plane_iso\",\n",
    "]].drop_duplicates(subset=[\"route_id\"]).copy()\n",
    "\n",
    "route_metrics_df = routes_raw[[\n",
    "    \"route_id\",\n",
    "    \"distance_km\",\n",
    "    \"distance_miles\",\n",
    "    \"is_international\",\n",
    "    \"co2_total_kg\",\n",
    "]].drop_duplicates(subset=[\"route_id\"]).copy()\n",
    "\n",
    "routes_ddl = \"\"\"\\\n",
    "-- routes table (Postgres)\n",
    "CREATE TABLE IF NOT EXISTS routes (\n",
    "  route_id               UUID PRIMARY KEY,\n",
    "  airline_id             TEXT NOT NULL,\n",
    "  source_airport_id      BIGINT NOT NULL,\n",
    "  destination_airport_id BIGINT NOT NULL,\n",
    "  stops                  INTEGER NOT NULL DEFAULT 0,\n",
    "  plane_iso              TEXT,\n",
    "\n",
    "  CONSTRAINT routes_airline_fk  FOREIGN KEY (airline_id) REFERENCES airlines(airline_id),\n",
    "  CONSTRAINT routes_source_fk   FOREIGN KEY (source_airport_id) REFERENCES airports(airport_id),\n",
    "  CONSTRAINT routes_dest_fk     FOREIGN KEY (destination_airport_id) REFERENCES airports(airport_id),\n",
    "  CONSTRAINT routes_plane_fk    FOREIGN KEY (plane_iso) REFERENCES aircraft_types(plane_iso)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS routes_src_idx  ON routes (source_airport_id);\n",
    "CREATE INDEX IF NOT EXISTS routes_dst_idx  ON routes (destination_airport_id);\n",
    "CREATE INDEX IF NOT EXISTS routes_airline_idx ON routes (airline_id);\n",
    "\n",
    "-- optional uniqueness on the natural key\n",
    "CREATE UNIQUE INDEX IF NOT EXISTS routes_natural_uq\n",
    "  ON routes (airline_id, source_airport_id, destination_airport_id, stops, plane_iso);\n",
    "\"\"\"\n",
    "\n",
    "route_metrics_ddl = \"\"\"\\\n",
    "-- route_metrics table (Postgres)\n",
    "CREATE TABLE IF NOT EXISTS route_metrics (\n",
    "  route_id        UUID PRIMARY KEY REFERENCES routes(route_id) ON DELETE CASCADE,\n",
    "  distance_km     DOUBLE PRECISION,\n",
    "  distance_miles  DOUBLE PRECISION,\n",
    "  is_international BOOLEAN,\n",
    "  co2_total_kg    DOUBLE PRECISION\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "routes_rows = []\n",
    "for r in routes_df.itertuples(index=False):\n",
    "    routes_rows.append(\n",
    "        \"INSERT INTO routes (route_id, airline_id, source_airport_id, destination_airport_id, stops, plane_iso)\\n\"\n",
    "        f\"VALUES ({sql_quote(r.route_id)}, {sql_quote(r.airline_id)}, {to_num(r.route_source_airport_id)}, \"\n",
    "        f\"{to_num(r.route_destination_airport_id)}, {to_num(r.route_stops)}, {sql_quote(r.route_plane_iso)});\\n\"\n",
    "    )\n",
    "routes_inserts = \"-- routes seed data\\n\" + \"\".join(routes_rows)\n",
    "\n",
    "metrics_rows = []\n",
    "for r in route_metrics_df.itertuples(index=False):\n",
    "    metrics_rows.append(\n",
    "        \"INSERT INTO route_metrics (route_id, distance_km, distance_miles, is_international, co2_total_kg)\\n\"\n",
    "        f\"VALUES ({sql_quote(r.route_id)}, {to_num(r.distance_km)}, {to_num(r.distance_miles)}, \"\n",
    "        f\"{to_bool(r.is_international)}, {to_num(r.co2_total_kg)});\\n\"\n",
    "    )\n",
    "metrics_inserts = \"-- route_metrics seed data\\n\" + \"\".join(metrics_rows)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"007_create_routes.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(routes_ddl)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"008_insert_routes.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(routes_inserts)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"009_create_route_metrics.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(route_metrics_ddl)\n",
    "\n",
    "with open(os.path.join(SQL_DIR, \"010_insert_route_metrics.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(metrics_inserts)\n",
    "\n",
    "print(f\"Wrote routes + route_metrics SQL to: {os.path.abspath(SQL_DIR)}\")\n",
    "print(f\"Routes rows: {len(routes_df):,} | Route metrics rows: {len(route_metrics_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8bff68",
   "metadata": {},
   "source": [
    "# Synthetic `flight_schedules` Generation Algorithm\n",
    "\n",
    "*(Demand + Reputation + Route Realism)*\n",
    "\n",
    "This document describes an **end-to-end synthetic schedule generator** that produces **Postgres `INSERT` SQL** for a `flight_schedules` table, using:\n",
    "\n",
    "* **Your main routes dataset** (`df`) with route / airline / airport / aircraft / metrics columns\n",
    "* A **city-pair frequency CSV** (e.g. `city_pairs.csv`) with columns:\n",
    "  `city_a, city_b, count`\n",
    "\n",
    "The generator is designed to be **explainable**, **tunable**, and **realism-oriented**.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Concept: What a “flight schedule” represents\n",
    "\n",
    "A `route` in your database means:\n",
    "\n",
    "> “This airline can fly A → B (possibly with stops) using aircraft type X.”\n",
    "\n",
    "A booking or search system needs something closer to reality: **repeating services** that operate weekly with departure times.\n",
    "\n",
    "A `flight_schedules` row represents a **repeating operating pattern**, for example:\n",
    "\n",
    "* Airline: BA\n",
    "* Route: JFK → LHR\n",
    "* Flight number: BA117\n",
    "* Departs: 19:35\n",
    "* Arrives: 07:25\n",
    "* Operates: Mon Tue Wed Thu Fri Sat Sun\n",
    "* Weekly frequency: 7\n",
    "\n",
    "If a route operates multiple times per day, we generate **multiple schedule rows**, not a single aggregated row.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Inputs and Pre-processing\n",
    "\n",
    "### 1.1 Market Definition: City-Pairs\n",
    "\n",
    "Each city-pair `(A, B)` is treated as a **market**.\n",
    "\n",
    "To avoid double counting `(A, B)` vs `(B, A)`, we canonicalize markets as:\n",
    "\n",
    "```\n",
    "market_key = tuple(sorted([city_a, city_b]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Aggregating Routes into Markets\n",
    "\n",
    "From `df`, we build the market key using:\n",
    "\n",
    "* `source_port_city`\n",
    "* `destination_port_city`\n",
    "\n",
    "This allows us to connect **city-pair demand** to the **set of available routes** that can serve that market.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Converting City-Pair Demand into Weekly Flights\n",
    "\n",
    "The `count` column in your city-pair CSV is treated as a **demand proxy**:\n",
    "\n",
    "* Higher count ⇒ busier market ⇒ more total departures per week\n",
    "\n",
    "We convert demand into total weekly flights using:\n",
    "\n",
    "* **Log normalization** to compress extreme values\n",
    "* A **calibration curve** mapping normalized demand into a frequency band\n",
    "\n",
    "The result is:\n",
    "\n",
    "```\n",
    "market_weekly_flights\n",
    "```\n",
    "\n",
    "This represents the **total number of departures per week across all airlines** in that market.\n",
    "\n",
    "This step is intentionally tunable so you can make the simulated network feel:\n",
    "\n",
    "* Denser (hub-heavy, competitive)\n",
    "* Sparser (regional, low-frequency)\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Airline Reputation / Strength Score\n",
    "\n",
    "To allocate more schedules to stronger airlines, we infer a **reputation proxy** directly from `df`.\n",
    "\n",
    "For each airline we compute:\n",
    "\n",
    "1. **Network size**\n",
    "   Number of routes operated\n",
    "   → Larger networks imply larger airlines\n",
    "\n",
    "2. **International share**\n",
    "   Mean of `is_international`\n",
    "   → International operations correlate with stronger carriers\n",
    "\n",
    "3. **Fleet capability proxy**\n",
    "   Share of high-capacity aircraft (`capacity_max`)\n",
    "   → Larger aircraft imply higher market strength\n",
    "\n",
    "4. **Efficiency / modernity proxy**\n",
    "   Lower `co2_g_per_pax_mile`\n",
    "   → Newer fleets tend to be more efficient\n",
    "\n",
    "Each metric is normalized to `[0, 1]` and combined into a single score.\n",
    "\n",
    "This score is mapped into a multiplicative factor:\n",
    "\n",
    "```\n",
    "reputation_factor ∈ [0.6 … 1.3]\n",
    "```\n",
    "\n",
    "The factor biases frequency allocation toward stronger airlines in competitive markets.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Route Realism Weights\n",
    "\n",
    "Within a market, multiple routes may compete. Each candidate route receives a **route weight**:\n",
    "\n",
    "```\n",
    "route_weight =\n",
    "    reputation_factor(airline)\n",
    "  × hub_bonus\n",
    "  × capacity_gauge_factor\n",
    "  × stops_penalty\n",
    "  × international_bias\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "### Hub Bonus\n",
    "\n",
    "Airlines fly more frequently from their hubs.\n",
    "\n",
    "Hubs are inferred from the share of an airline’s routes originating in each city.\n",
    "\n",
    "---\n",
    "\n",
    "### Capacity Gauge Factor\n",
    "\n",
    "Higher-capacity aircraft slightly reduce the need for frequency but also signal strength.\n",
    "\n",
    "We apply a **square-root scaling** to keep effects soft and realistic.\n",
    "\n",
    "---\n",
    "\n",
    "### Stops Penalty\n",
    "\n",
    "Multi-stop routes receive lower frequency allocations.\n",
    "\n",
    "---\n",
    "\n",
    "### International Bias\n",
    "\n",
    "International routes typically operate less frequently than dense domestic shuttles.\n",
    "\n",
    "---\n",
    "\n",
    "All of these adjustments are **gentle modifiers** — demand and airline reputation remain the dominant drivers.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Allocating Market Flights to Routes\n",
    "\n",
    "For each market:\n",
    "\n",
    "1. Compute `market_weekly_flights` from demand\n",
    "2. Identify all candidate routes in `df` that serve the city-pair (either direction)\n",
    "3. Compute `route_weight` for each candidate\n",
    "4. Convert weights into shares and allocate integer frequencies using **largest-remainder rounding**:\n",
    "\n",
    "   * Assign `floor(share × market_weekly_flights)`\n",
    "   * Distribute remaining flights to routes with the largest fractional remainders\n",
    "\n",
    "**Result:**\n",
    "Each `(airline, route)` receives an integer `allocated_weekly_flights`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Expanding Weekly Flights into Schedule Rows\n",
    "\n",
    "A route may be allocated, for example, 20 flights per week.\n",
    "\n",
    "Instead of one row with `20`, we generate **multiple schedule rows**:\n",
    "\n",
    "1. Allocate as many **daily (7/wk)** schedules as possible\n",
    "2. Allocate remaining flights as **partial-week schedules** (e.g. 3/wk, 5/wk)\n",
    "\n",
    "For each schedule row we generate:\n",
    "\n",
    "### Operating Days\n",
    "\n",
    "A 7-bit mask (Mon–Sun):\n",
    "\n",
    "* Daily: `127` (`1111111`)\n",
    "* Weekdays: `31` (`0011111`)\n",
    "* Remainders: random valid subsets\n",
    "\n",
    "---\n",
    "\n",
    "### Departure Time\n",
    "\n",
    "Based on distance:\n",
    "\n",
    "* Short-haul: morning and evening peaks\n",
    "* Long-haul: midday and red-eye bias\n",
    "\n",
    "---\n",
    "\n",
    "### Arrival Time\n",
    "\n",
    "Estimated using a simple block-time model:\n",
    "\n",
    "```\n",
    "duration_hours ≈ distance_km / cruise_speed + overhead\n",
    "arrival_time = depart_time + duration\n",
    "```\n",
    "\n",
    "Time zones are **intentionally ignored** for now to keep the model simple.\n",
    "\n",
    "---\n",
    "\n",
    "### Flight Number\n",
    "\n",
    "Deterministically derived from:\n",
    "\n",
    "* Airline code\n",
    "* Route hash\n",
    "\n",
    "Ensures stability across regeneration runs.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Output: SQL Insert File\n",
    "\n",
    "The generator writes **Postgres INSERT statements only** (no DB writes from Python) to:\n",
    "\n",
    "```\n",
    "./sql_statements/011_generate_flight_schedules.sql\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Explicit Assumptions & Limitations\n",
    "\n",
    "* Schedules are **synthetic** and plausible, not real-world replicas\n",
    "* Arrival times ignore timezone differences\n",
    "* City-pairs not present in `df` are skipped\n",
    "* Routes with no demand entry receive a small default demand\n",
    "\n",
    "---\n",
    "\n",
    "## 9) How to Tune the Generator\n",
    "\n",
    "Key parameters you can adjust in code:\n",
    "\n",
    "* `MIN_MARKET_WEEKLY`, `MAX_MARKET_WEEKLY` — overall network density\n",
    "* Demand curve parameters (`DEMAND_ALPHA`, `DEMAND_BETA`)\n",
    "* Reputation weights (`REP_W_*`)\n",
    "* Penalties (stops, international bias)\n",
    "* Schedule splitting rules (daily vs partial-week patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a33fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote flight_schedules INSERT SQL: /Users/darius/Documents/projects/portfolio/flighter/data-mining/sql_statements/011_generate_flight_schedules.sql\n",
      "Generated schedules: 56,919\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_schedule_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>airline_id</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>depart_time_local</th>\n",
       "      <th>arrive_time_local</th>\n",
       "      <th>operating_days_mask</th>\n",
       "      <th>weekly_frequency</th>\n",
       "      <th>aircraft_iso</th>\n",
       "      <th>effective_from</th>\n",
       "      <th>effective_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d12306b6-dde3-599d-b21e-1e27b3fb4de1</td>\n",
       "      <td>2815ac1f-e72d-52be-af25-841356236ef7</td>\n",
       "      <td>3674</td>\n",
       "      <td>DD2278</td>\n",
       "      <td>06:26</td>\n",
       "      <td>07:58</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>738</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18e9b1bb-28ca-52ce-a843-4318440c38b6</td>\n",
       "      <td>df01f2a1-57f0-5b43-9897-c1f2170e75d4</td>\n",
       "      <td>3674</td>\n",
       "      <td>DD5529</td>\n",
       "      <td>16:44</td>\n",
       "      <td>18:16</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>738</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7b985621-d2b7-5a18-bafc-b40798214e9a</td>\n",
       "      <td>cd263f06-d50c-547d-af99-ddc1d5a4e162</td>\n",
       "      <td>4947</td>\n",
       "      <td>FD3445</td>\n",
       "      <td>16:16</td>\n",
       "      <td>17:48</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>320</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43cb51e9-4630-5b66-b009-80ba72ebea05</td>\n",
       "      <td>781a81ac-a804-5ff5-9501-317a87226adb</td>\n",
       "      <td>4947</td>\n",
       "      <td>FD8110</td>\n",
       "      <td>06:55</td>\n",
       "      <td>08:27</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>320</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6999b418-6f83-5bb1-b272-d551f6ee15c6</td>\n",
       "      <td>43b07bce-78ce-59d2-bd36-1ee98fa441c0</td>\n",
       "      <td>2987</td>\n",
       "      <td>JL8117</td>\n",
       "      <td>08:34</td>\n",
       "      <td>10:09</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>734</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5b6caca3-13e4-559f-9b80-235981fabdfc</td>\n",
       "      <td>919b3b4d-500f-590c-b7a8-fe13fe3754d8</td>\n",
       "      <td>2987</td>\n",
       "      <td>JL4663</td>\n",
       "      <td>06:50</td>\n",
       "      <td>08:25</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>734</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4d59f162-3d6d-586a-925a-0bd751bb2b04</td>\n",
       "      <td>622fd061-37c7-5d98-bf83-9efa70dab28f</td>\n",
       "      <td>3090</td>\n",
       "      <td>KL5802</td>\n",
       "      <td>16:03</td>\n",
       "      <td>17:38</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>320</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3824ba8b-7d0e-5faa-9bbc-f8573ff1515a</td>\n",
       "      <td>38078b64-5db3-5add-a832-7b26e455ce18</td>\n",
       "      <td>3090</td>\n",
       "      <td>KL5049</td>\n",
       "      <td>09:15</td>\n",
       "      <td>10:50</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>320</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>176490ec-5f6c-57dd-80a3-654a26a10338</td>\n",
       "      <td>86e8746d-4909-5eeb-b608-5d2f5ebcf8af</td>\n",
       "      <td>3378</td>\n",
       "      <td>MH4857</td>\n",
       "      <td>06:23</td>\n",
       "      <td>07:58</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>320</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b8b4a37b-385a-56b4-a587-e4a070a68c09</td>\n",
       "      <td>bc41150c-dcc5-5395-9940-edfdb026f219</td>\n",
       "      <td>3378</td>\n",
       "      <td>MH6013</td>\n",
       "      <td>07:31</td>\n",
       "      <td>09:06</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>320</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>2026-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     flight_schedule_id                              route_id  \\\n",
       "0  d12306b6-dde3-599d-b21e-1e27b3fb4de1  2815ac1f-e72d-52be-af25-841356236ef7   \n",
       "1  18e9b1bb-28ca-52ce-a843-4318440c38b6  df01f2a1-57f0-5b43-9897-c1f2170e75d4   \n",
       "2  7b985621-d2b7-5a18-bafc-b40798214e9a  cd263f06-d50c-547d-af99-ddc1d5a4e162   \n",
       "3  43cb51e9-4630-5b66-b009-80ba72ebea05  781a81ac-a804-5ff5-9501-317a87226adb   \n",
       "4  6999b418-6f83-5bb1-b272-d551f6ee15c6  43b07bce-78ce-59d2-bd36-1ee98fa441c0   \n",
       "5  5b6caca3-13e4-559f-9b80-235981fabdfc  919b3b4d-500f-590c-b7a8-fe13fe3754d8   \n",
       "6  4d59f162-3d6d-586a-925a-0bd751bb2b04  622fd061-37c7-5d98-bf83-9efa70dab28f   \n",
       "7  3824ba8b-7d0e-5faa-9bbc-f8573ff1515a  38078b64-5db3-5add-a832-7b26e455ce18   \n",
       "8  176490ec-5f6c-57dd-80a3-654a26a10338  86e8746d-4909-5eeb-b608-5d2f5ebcf8af   \n",
       "9  b8b4a37b-385a-56b4-a587-e4a070a68c09  bc41150c-dcc5-5395-9940-edfdb026f219   \n",
       "\n",
       "   airline_id flight_number depart_time_local arrive_time_local  \\\n",
       "0        3674        DD2278             06:26             07:58   \n",
       "1        3674        DD5529             16:44             18:16   \n",
       "2        4947        FD3445             16:16             17:48   \n",
       "3        4947        FD8110             06:55             08:27   \n",
       "4        2987        JL8117             08:34             10:09   \n",
       "5        2987        JL4663             06:50             08:25   \n",
       "6        3090        KL5802             16:03             17:38   \n",
       "7        3090        KL5049             09:15             10:50   \n",
       "8        3378        MH4857             06:23             07:58   \n",
       "9        3378        MH6013             07:31             09:06   \n",
       "\n",
       "   operating_days_mask  weekly_frequency aircraft_iso effective_from  \\\n",
       "0                   95                 6          738     2026-01-01   \n",
       "1                  127                 7          738     2026-01-01   \n",
       "2                  127                 7          320     2026-01-01   \n",
       "3                  127                 7          320     2026-01-01   \n",
       "4                  127                 7          734     2026-01-01   \n",
       "5                  127                 7          734     2026-01-01   \n",
       "6                   95                 6          320     2026-01-01   \n",
       "7                   63                 6          320     2026-01-01   \n",
       "8                  127                 7          320     2026-01-01   \n",
       "9                  127                 7          320     2026-01-01   \n",
       "\n",
       "  effective_to  \n",
       "0   2026-12-31  \n",
       "1   2026-12-31  \n",
       "2   2026-12-31  \n",
       "3   2026-12-31  \n",
       "4   2026-12-31  \n",
       "5   2026-12-31  \n",
       "6   2026-12-31  \n",
       "7   2026-12-31  \n",
       "8   2026-12-31  \n",
       "9   2026-12-31  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "SQL_DIR = \"./sql_statements\"\n",
    "os.makedirs(SQL_DIR, exist_ok=True)\n",
    "\n",
    "SCRIPT_NAME = \"generate_flight_schedules\"\n",
    "OUT_SQL_PATH = os.path.join(SQL_DIR, f\"011_{SCRIPT_NAME}.sql\")\n",
    "\n",
    "# Path to your city-pair demand CSV:\n",
    "# Must have columns: city_a, city_b, count\n",
    "CITYPAIR_CSV_PATH = \"./clean_data/busiest_city_pairs.csv\"\n",
    "\n",
    "# Effective date window for generated schedules (synthetic, tunable)\n",
    "EFFECTIVE_FROM = date(2026, 1, 1).isoformat()\n",
    "EFFECTIVE_TO   = date(2026, 12, 31).isoformat()\n",
    "\n",
    "# Demand → weekly flights tuning\n",
    "MIN_MARKET_WEEKLY = 2     # smallest markets: 2 flights/week\n",
    "MAX_MARKET_WEEKLY = 140   # biggest markets: up to 20/day across all airlines (synthetic upper bound)\n",
    "\n",
    "# Curve parameters:\n",
    "# demand_norm in [0,1]\n",
    "# market_weekly = MIN + (MAX-MIN) * (demand_norm ** DEMAND_ALPHA) * (0.5 + 0.5*demand_norm) ** DEMAND_BETA\n",
    "DEMAND_ALPHA = 0.55\n",
    "DEMAND_BETA  = 0.65\n",
    "\n",
    "# Airline reputation weights (sum roughly 1.0)\n",
    "REP_W_NETWORK = 0.50\n",
    "REP_W_INTL    = 0.20\n",
    "REP_W_CAP     = 0.20\n",
    "REP_W_ECO     = 0.10\n",
    "\n",
    "# Route weight modifiers\n",
    "STOPS_PENALTY_PER_STOP = 0.35     # each stop reduces attractiveness\n",
    "INTL_FREQ_MULTIPLIER   = 0.75     # international routes tend to be less frequent\n",
    "HUB_BONUS_MULTIPLIER   = 0.30     # hub effect strength\n",
    "GAUGE_SOFTENING        = 0.35     # how strongly capacity influences weight (very gentle)\n",
    "\n",
    "# Schedule generation\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Cruise speed assumptions for time estimation (km/h)\n",
    "CRUISE_SPEED_SHORT = 720\n",
    "CRUISE_SPEED_MED   = 800\n",
    "CRUISE_SPEED_LONG  = 860\n",
    "OVERHEAD_HOURS     = 0.6  # taxi/climb/descend etc.\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def sql_quote(v):\n",
    "    \"\"\"SQL literal quoting for text/date/time.\"\"\"\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    s = str(v).replace(\"'\", \"''\")\n",
    "    return f\"'{s}'\"\n",
    "\n",
    "def to_num(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    if isinstance(v, (np.integer, int)):\n",
    "        return str(int(v))\n",
    "    if isinstance(v, (np.floating, float)):\n",
    "        # avoid scientific notation in SQL\n",
    "        return format(float(v), \"f\").rstrip(\"0\").rstrip(\".\") if \".\" in format(float(v), \"f\") else str(float(v))\n",
    "    return str(v)\n",
    "\n",
    "def to_bool(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or (pd.isna(v) if not isinstance(v, str) else False):\n",
    "        return \"NULL\"\n",
    "    return \"TRUE\" if bool(v) else \"FALSE\"\n",
    "\n",
    "def normalize_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Min-max normalize to [0,1], safe for constant series.\"\"\"\n",
    "    s = s.astype(float)\n",
    "    mn, mx = float(s.min()), float(s.max())\n",
    "    if math.isclose(mn, mx):\n",
    "        return pd.Series([0.5] * len(s), index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "def stable_uuid_from_key(key: str) -> str:\n",
    "    \"\"\"Deterministic UUID from a string key.\"\"\"\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_URL, key))\n",
    "\n",
    "def stable_int_from_key(key: str) -> int:\n",
    "    \"\"\"Deterministic int from a string key (for flight numbers).\"\"\"\n",
    "    h = hashlib.sha1(key.encode(\"utf-8\")).digest()\n",
    "    return int.from_bytes(h[:8], \"big\", signed=False)\n",
    "\n",
    "def stable_route_id(airline_id, src_airport_id, dst_airport_id, stops, plane_iso) -> str:\n",
    "    \"\"\"Deterministic route_id derived from the natural keys (UUID).\"\"\"\n",
    "    key = f\"ROUTE|{airline_id}|{src_airport_id}|{dst_airport_id}|{stops}|{plane_iso}\"\n",
    "    return stable_uuid_from_key(key)\n",
    "\n",
    "def canonical_citypair(a: str, b: str) -> tuple:\n",
    "    \"\"\"Canonical market key for a pair of cities (unordered).\"\"\"\n",
    "    if a is None or b is None or pd.isna(a) or pd.isna(b):\n",
    "        return (None, None)\n",
    "    a = str(a).strip()\n",
    "    b = str(b).strip()\n",
    "    return tuple(sorted([a, b]))\n",
    "\n",
    "def demand_to_market_weekly(demand_norm: float) -> int:\n",
    "    \"\"\"Map normalized demand [0,1] to a plausible market weekly flight total.\"\"\"\n",
    "    demand_norm = float(np.clip(demand_norm, 0.0, 1.0))\n",
    "    val = MIN_MARKET_WEEKLY + (MAX_MARKET_WEEKLY - MIN_MARKET_WEEKLY) * \\\n",
    "          (demand_norm ** DEMAND_ALPHA) * ((0.5 + 0.5 * demand_norm) ** DEMAND_BETA)\n",
    "    return int(round(val))\n",
    "\n",
    "def pick_operating_days_mask(k: int) -> int:\n",
    "    \"\"\"\n",
    "    Choose k operating days out of 7 as a bitmask (Mon..Sun => bits 0..6).\n",
    "    Example: daily => 127 (0b1111111)\n",
    "    \"\"\"\n",
    "    k = int(max(1, min(7, k)))\n",
    "    days = list(range(7))  # 0=Mon ... 6=Sun\n",
    "    random.shuffle(days)\n",
    "    chosen = sorted(days[:k])\n",
    "    mask = 0\n",
    "    for d in chosen:\n",
    "        mask |= (1 << d)\n",
    "    return mask\n",
    "\n",
    "def time_to_str(minutes_since_midnight: int) -> str:\n",
    "    minutes_since_midnight %= (24 * 60)\n",
    "    hh = minutes_since_midnight // 60\n",
    "    mm = minutes_since_midnight % 60\n",
    "    return f\"{hh:02d}:{mm:02d}\"\n",
    "\n",
    "def estimate_block_time_minutes(distance_km: float) -> int:\n",
    "    \"\"\"Estimate block time (minutes) from distance_km with crude cruise speed bands.\"\"\"\n",
    "    if distance_km is None or pd.isna(distance_km) or distance_km <= 0:\n",
    "        return int(round((1.5 + OVERHEAD_HOURS) * 60))\n",
    "    if distance_km < 800:\n",
    "        speed = CRUISE_SPEED_SHORT\n",
    "    elif distance_km < 3500:\n",
    "        speed = CRUISE_SPEED_MED\n",
    "    else:\n",
    "        speed = CRUISE_SPEED_LONG\n",
    "    hours = (float(distance_km) / speed) + OVERHEAD_HOURS\n",
    "    return int(round(hours * 60))\n",
    "\n",
    "def choose_departure_minutes(distance_km: float, is_international: bool) -> int:\n",
    "    \"\"\"\n",
    "    Choose a plausible departure time (minutes since midnight) based on distance and intl.\n",
    "    - short haul: peaks around 06-09 and 16-20\n",
    "    - medium haul: spread across day\n",
    "    - long haul: midday and evening/red-eye bias\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if distance_km is None or pd.isna(distance_km):\n",
    "        distance_km = 1000\n",
    "\n",
    "    if distance_km < 800 and not is_international:\n",
    "        # commuter peaks\n",
    "        if r < 0.55:\n",
    "            # morning peak\n",
    "            return random.randint(6 * 60, 9 * 60 + 30)\n",
    "        else:\n",
    "            # evening peak\n",
    "            return random.randint(16 * 60, 20 * 60 + 30)\n",
    "    elif distance_km < 3500:\n",
    "        # spread\n",
    "        if r < 0.20:\n",
    "            return random.randint(6 * 60, 9 * 60 + 30)\n",
    "        elif r < 0.70:\n",
    "            return random.randint(10 * 60, 16 * 60 + 30)\n",
    "        else:\n",
    "            return random.randint(17 * 60, 22 * 60)\n",
    "    else:\n",
    "        # long haul: midday + evening/red-eye\n",
    "        if r < 0.45:\n",
    "            return random.randint(11 * 60, 15 * 60 + 30)\n",
    "        else:\n",
    "            return random.randint(18 * 60, 23 * 60 + 30)\n",
    "\n",
    "def make_flight_number(airline_iata: str, airline_icao: str, key: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic-ish flight number:\n",
    "    prefer IATA (2 chars), else ICAO (3 chars), else 'XX'\n",
    "    number: 3-4 digits derived from hash\n",
    "    \"\"\"\n",
    "    prefix = None\n",
    "    if isinstance(airline_iata, str) and airline_iata.strip():\n",
    "        prefix = airline_iata.strip().upper()\n",
    "    elif isinstance(airline_icao, str) and airline_icao.strip():\n",
    "        prefix = airline_icao.strip().upper()\n",
    "    else:\n",
    "        prefix = \"XX\"\n",
    "\n",
    "    n = stable_int_from_key(key) % 9000 + 100  # 100..9099\n",
    "    return f\"{prefix}{n}\"\n",
    "\n",
    "def largest_remainder_allocation(total: int, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Allocate integer counts that sum to total, proportional to weights.\n",
    "    Uses largest remainder method.\n",
    "    \"\"\"\n",
    "    total = int(max(0, total))\n",
    "    if total == 0 or weights.sum() <= 0:\n",
    "        return np.zeros_like(weights, dtype=int)\n",
    "\n",
    "    shares = weights / weights.sum()\n",
    "    raw = shares * total\n",
    "    base = np.floor(raw).astype(int)\n",
    "    remainder = raw - base\n",
    "\n",
    "    deficit = total - base.sum()\n",
    "    if deficit > 0:\n",
    "        idx = np.argsort(-remainder)  # descending remainder\n",
    "        for i in idx[:deficit]:\n",
    "            base[i] += 1\n",
    "    return base\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load and normalize demand table\n",
    "# ----------------------------\n",
    "citypairs = pd.read_csv(CITYPAIR_CSV_PATH)\n",
    "required_cols = {\"city_a\", \"city_b\", \"count\"}\n",
    "missing = required_cols - set(citypairs.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"City-pair CSV missing columns: {missing}\")\n",
    "\n",
    "citypairs[\"city_a\"] = citypairs[\"city_a\"].astype(\"string\").str.strip()\n",
    "citypairs[\"city_b\"] = citypairs[\"city_b\"].astype(\"string\").str.strip()\n",
    "citypairs[\"market_key\"] = citypairs.apply(lambda r: canonical_citypair(r[\"city_a\"], r[\"city_b\"]), axis=1)\n",
    "\n",
    "# Combine duplicates safely\n",
    "demand_by_market = (\n",
    "    citypairs.groupby(\"market_key\", as_index=True)[\"count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Normalize demand with log scaling to reduce extreme skew\n",
    "log_demand = np.log1p(demand_by_market.astype(float))\n",
    "demand_norm = normalize_series(log_demand)\n",
    "\n",
    "market_weekly = demand_norm.apply(demand_to_market_weekly).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build route candidates with market keys from df\n",
    "# ----------------------------\n",
    "routes = df.copy()\n",
    "\n",
    "# Canonical market key by cities\n",
    "routes[\"market_key\"] = routes.apply(\n",
    "    lambda r: canonical_citypair(r.get(\"source_port_city\"), r.get(\"destination_port_city\")),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Deterministic route_id (so schedules can reference it)\n",
    "routes[\"route_id\"] = routes.apply(\n",
    "    lambda r: stable_route_id(\n",
    "        r.get(\"airline_id\"),\n",
    "        r.get(\"route_source_airport_id\"),\n",
    "        r.get(\"route_destination_airport_id\"),\n",
    "        r.get(\"route_stops\"),\n",
    "        r.get(\"route_plane_iso\"),\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# If a market appears in routes but not in demand CSV, give it a small default demand\n",
    "# (this prevents \"dead markets\" if your CSV doesn't cover everything)\n",
    "all_markets_in_routes = set(routes[\"market_key\"].dropna().unique().tolist())\n",
    "missing_markets = all_markets_in_routes - set(market_weekly.index.tolist())\n",
    "if missing_markets:\n",
    "    # default demand is low: around 2-6 flights/week depending on distance\n",
    "    # we implement default as a small constant market_weekly = MIN_MARKET_WEEKLY\n",
    "    # (you can tune this)\n",
    "    for mk in missing_markets:\n",
    "        market_weekly.loc[mk] = MIN_MARKET_WEEKLY\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Compute airline reputation factor from df (proxy)\n",
    "# ----------------------------\n",
    "# Network size\n",
    "routes_per_airline = routes.groupby(\"airline_id\").size()\n",
    "\n",
    "# International share\n",
    "intl_share = routes.groupby(\"airline_id\")[\"is_international\"].mean().fillna(0.0)\n",
    "\n",
    "# Fleet capability proxy: share of routes with capacity_max > 250 (widebody-ish)\n",
    "is_wide = (routes[\"capacity_max\"].fillna(0) > 250).astype(int)\n",
    "wide_share = routes.assign(is_wide=is_wide).groupby(\"airline_id\")[\"is_wide\"].mean().fillna(0.0)\n",
    "\n",
    "# Eco proxy: lower CO2 per pax-mile is better\n",
    "eco_raw = routes.groupby(\"airline_id\")[\"co2_g_per_pax_mile\"].mean()\n",
    "eco_norm = 1.0 - normalize_series(eco_raw.fillna(eco_raw.median() if not eco_raw.dropna().empty else 0.0))\n",
    "\n",
    "rep = (\n",
    "    REP_W_NETWORK * normalize_series(routes_per_airline) +\n",
    "    REP_W_INTL    * normalize_series(intl_share) +\n",
    "    REP_W_CAP     * normalize_series(wide_share) +\n",
    "    REP_W_ECO     * eco_norm\n",
    ").clip(0.0, 1.0)\n",
    "\n",
    "# map to multiplicative factor\n",
    "reputation_factor = (0.6 + 0.7 * rep).to_dict()  # 0.6..1.3\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Infer hubs: hub strength per (airline, source_city)\n",
    "# ----------------------------\n",
    "airline_city_origin = routes.groupby([\"airline_id\", \"source_port_city\"]).size()\n",
    "airline_total = routes.groupby(\"airline_id\").size()\n",
    "\n",
    "hub_strength = (airline_city_origin / airline_city_origin.index.get_level_values(0).map(airline_total)).fillna(0.0)\n",
    "# hub_strength is in [0,1], higher means \"this city is a hub for this airline\"\n",
    "\n",
    "def get_hub_bonus(airline_id, source_city) -> float:\n",
    "    try:\n",
    "        hs = float(hub_strength.loc[(airline_id, source_city)])\n",
    "    except Exception:\n",
    "        hs = 0.0\n",
    "    return 1.0 + HUB_BONUS_MULTIPLIER * hs  # mild boost\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Allocate market weekly flights → route-level frequencies\n",
    "# ----------------------------\n",
    "schedule_rows = []\n",
    "\n",
    "# Pre-group routes by market for speed\n",
    "routes_by_market = {mk: g for mk, g in routes.groupby(\"market_key\")}\n",
    "\n",
    "for mk, total_weekly in market_weekly.items():\n",
    "    if mk not in routes_by_market:\n",
    "        continue\n",
    "\n",
    "    cand = routes_by_market[mk].copy()\n",
    "    if cand.empty:\n",
    "        continue\n",
    "\n",
    "    # Compute route weights\n",
    "    # reputation: per airline\n",
    "    cand[\"rep_factor\"] = cand[\"airline_id\"].map(lambda x: reputation_factor.get(x, 1.0))\n",
    "\n",
    "    # hub bonus uses source city\n",
    "    cand[\"hub_bonus\"] = cand.apply(lambda r: get_hub_bonus(r[\"airline_id\"], r[\"source_port_city\"]), axis=1)\n",
    "\n",
    "    # stops penalty\n",
    "    stops = cand[\"route_stops\"].fillna(0).astype(int)\n",
    "    cand[\"stops_factor\"] = (1.0 / (1.0 + STOPS_PENALTY_PER_STOP * stops)).clip(0.1, 1.0)\n",
    "\n",
    "    # international frequency multiplier (applied as a mild weight reduction)\n",
    "    cand[\"intl_factor\"] = cand[\"is_international\"].fillna(False).astype(bool).map(\n",
    "        lambda x: INTL_FREQ_MULTIPLIER if x else 1.0\n",
    "    )\n",
    "\n",
    "    # gauge factor: we want this gentle; higher capacity slightly increases \"ability\" but not linearly\n",
    "    cap = cand[\"capacity_max\"].fillna(cand[\"capacity_max\"].median() if not cand[\"capacity_max\"].dropna().empty else 150)\n",
    "    cap = cap.clip(lower=50)\n",
    "    cand[\"gauge_factor\"] = (cap ** GAUGE_SOFTENING) / float((cap ** GAUGE_SOFTENING).median())\n",
    "\n",
    "    cand[\"route_weight\"] = (\n",
    "        cand[\"rep_factor\"] *\n",
    "        cand[\"hub_bonus\"] *\n",
    "        cand[\"stops_factor\"] *\n",
    "        cand[\"intl_factor\"] *\n",
    "        cand[\"gauge_factor\"]\n",
    "    ).astype(float)\n",
    "\n",
    "    weights = cand[\"route_weight\"].to_numpy(dtype=float)\n",
    "    alloc = largest_remainder_allocation(int(total_weekly), weights)\n",
    "\n",
    "    cand = cand.assign(allocated_weekly=alloc)\n",
    "    cand = cand[cand[\"allocated_weekly\"] > 0]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6) Expand allocations into multiple schedule rows\n",
    "    # ----------------------------\n",
    "    for r in cand.itertuples(index=False):\n",
    "        remaining = int(r.allocated_weekly)\n",
    "\n",
    "        # decide how many daily schedules (7/wk) to create\n",
    "        daily_count = remaining // 7\n",
    "        remainder = remaining % 7\n",
    "\n",
    "        # if very large, allow multiple daily schedules (e.g., 21/wk => 3 daily)\n",
    "        schedule_chunks = [7] * daily_count\n",
    "        if remainder > 0:\n",
    "            schedule_chunks.append(remainder)\n",
    "\n",
    "        # If the market is very small (1-2/week), keep it as one partial schedule\n",
    "        # already covered by chunks\n",
    "\n",
    "        for chunk_idx, wf in enumerate(schedule_chunks):\n",
    "            # operating days mask\n",
    "            if wf >= 7:\n",
    "                op_mask = 127  # 1111111\n",
    "            else:\n",
    "                # for partial, choose wf days\n",
    "                op_mask = pick_operating_days_mask(wf)\n",
    "\n",
    "            # derive times\n",
    "            dist_km = r.distance_km\n",
    "            intl = bool(r.is_international) if not pd.isna(r.is_international) else False\n",
    "\n",
    "            dep_min = choose_departure_minutes(dist_km, intl)\n",
    "            block_min = estimate_block_time_minutes(dist_km)\n",
    "            arr_min = dep_min + block_min  # timezone differences ignored (explicit)\n",
    "\n",
    "            dep_time = time_to_str(dep_min)\n",
    "            arr_time = time_to_str(arr_min)\n",
    "\n",
    "            # flight number (stable-ish)\n",
    "            fn_key = f\"{r.airline_id}|{r.route_id}|{chunk_idx}|{dep_time}|{op_mask}\"\n",
    "            flight_number = make_flight_number(r.airline_iata, r.airline_icao, fn_key)\n",
    "\n",
    "            # schedule_id (deterministic UUID)\n",
    "            schedule_id = stable_uuid_from_key(f\"SCHED|{fn_key}\")\n",
    "\n",
    "            schedule_rows.append({\n",
    "                \"flight_schedule_id\": schedule_id,\n",
    "                \"route_id\": r.route_id,\n",
    "                \"airline_id\": r.airline_id,\n",
    "                \"flight_number\": flight_number,\n",
    "                \"depart_time_local\": dep_time,\n",
    "                \"arrive_time_local\": arr_time,\n",
    "                \"operating_days_mask\": int(op_mask),\n",
    "                \"weekly_frequency\": int(wf),\n",
    "                \"aircraft_iso\": r.route_plane_iso,\n",
    "                \"effective_from\": EFFECTIVE_FROM,\n",
    "                \"effective_to\": EFFECTIVE_TO,\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame and de-duplicate just in case\n",
    "schedules_df = pd.DataFrame(schedule_rows).drop_duplicates(subset=[\"flight_schedule_id\"]).copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Generate SQL INSERT file\n",
    "# ----------------------------\n",
    "flight_schedules_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS flight_schedules (\n",
    "    flight_schedule_id UUID PRIMARY KEY,\n",
    "\n",
    "    route_id   UUID NOT NULL,\n",
    "    airline_id TEXT NOT NULL,\n",
    "\n",
    "    flight_number VARCHAR(8) NOT NULL,\n",
    "\n",
    "    depart_time_local TIME NOT NULL,\n",
    "    arrive_time_local TIME NOT NULL,\n",
    "\n",
    "    operating_days_mask SMALLINT NOT NULL\n",
    "        CHECK (operating_days_mask BETWEEN 1 AND 127),\n",
    "\n",
    "    weekly_frequency SMALLINT NOT NULL\n",
    "        CHECK (weekly_frequency > 0 AND weekly_frequency <= 21),\n",
    "\n",
    "    aircraft_iso CHAR(3) NOT NULL,\n",
    "\n",
    "    effective_from DATE NOT NULL,\n",
    "    effective_to   DATE NOT NULL,\n",
    "\n",
    "    CONSTRAINT chk_effective_dates\n",
    "        CHECK (effective_to >= effective_from)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql_lines = []\n",
    "sql_lines.append(f\"-- Synthetic flight schedules generated from demand + reputation + route realism\")\n",
    "sql_lines.append(f\"-- Source demand file: {CITYPAIR_CSV_PATH}\")\n",
    "sql_lines.append(f\"-- Output date range: {EFFECTIVE_FROM} to {EFFECTIVE_TO}\")\n",
    "sql_lines.append(f\"-- Rows: {len(schedules_df):,}\")\n",
    "sql_lines.append(\"\")\n",
    "\n",
    "sql_lines.append(flight_schedules_ddl + \"\\n\\n\")\n",
    "\n",
    "for r in schedules_df.itertuples(index=False):\n",
    "    sql_lines.append(\n",
    "        \"INSERT INTO flight_schedules \"\n",
    "        \"(flight_schedule_id, route_id, airline_id, flight_number, depart_time_local, arrive_time_local, \"\n",
    "        \"operating_days_mask, weekly_frequency, aircraft_iso, effective_from, effective_to)\\n\"\n",
    "        f\"VALUES ({sql_quote(r.flight_schedule_id)}, {sql_quote(r.route_id)}, {sql_quote(r.airline_id)}, \"\n",
    "        f\"{sql_quote(r.flight_number)}, {sql_quote(r.depart_time_local)}, {sql_quote(r.arrive_time_local)}, \"\n",
    "        f\"{to_num(r.operating_days_mask)}, {to_num(r.weekly_frequency)}, {sql_quote(r.aircraft_iso)}, \"\n",
    "        f\"{sql_quote(r.effective_from)}, {sql_quote(r.effective_to)});\\n\"\n",
    "    )\n",
    "\n",
    "with open(OUT_SQL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sql_lines))\n",
    "\n",
    "print(f\"✅ Wrote flight_schedules INSERT SQL: {os.path.abspath(OUT_SQL_PATH)}\")\n",
    "print(f\"Generated schedules: {len(schedules_df):,}\")\n",
    "print(\"Sample rows:\")\n",
    "display(schedules_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
